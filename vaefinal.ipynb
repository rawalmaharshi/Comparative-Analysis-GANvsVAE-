{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vaedec12.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpDS2LVpX7CK"
      },
      "source": [
        "# prerequisites\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "bs = 100\n",
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ura2uUneYLAh"
      },
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        \n",
        "        # encoder part\n",
        "        self.fc1 = nn.Linear(x_dim, h_dim1)\n",
        "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
        "        self.fc31 = nn.Linear(h_dim2, z_dim)\n",
        "        self.fc32 = nn.Linear(h_dim2, z_dim)\n",
        "        # decoder part\n",
        "        self.fc4 = nn.Linear(z_dim, h_dim2)\n",
        "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
        "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
        "        \n",
        "    def encoder(self, x):\n",
        "        h = F.relu(self.fc1(x))\n",
        "        h = F.relu(self.fc2(h))\n",
        "        return self.fc31(h), self.fc32(h) # mu, log_var\n",
        "    \n",
        "    def sampling(self, mu, log_var):\n",
        "        std = torch.exp(0.5*log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps.mul(std).add_(mu) # return z sample\n",
        "        \n",
        "    def decoder(self, z):\n",
        "        h = F.relu(self.fc4(z))\n",
        "        h = F.relu(self.fc5(h))\n",
        "        return F.sigmoid(self.fc6(h)) \n",
        "    \n",
        "    def forward(self, x):\n",
        "        mu, log_var = self.encoder(x.view(-1, 784))\n",
        "        z = self.sampling(mu, log_var)\n",
        "        return self.decoder(z), mu, log_var\n",
        "\n",
        "# build model\n",
        "vae = VAE(x_dim=784, h_dim1= 512, h_dim2=256, z_dim=2)\n",
        "if torch.cuda.is_available():\n",
        "    vae.cuda()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUNnT7dZYOLB",
        "outputId": "f6feb930-7ef3-40d3-fae0-c634a8e3560a"
      },
      "source": [
        "vae"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VAE(\n",
              "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (fc31): Linear(in_features=256, out_features=2, bias=True)\n",
              "  (fc32): Linear(in_features=256, out_features=2, bias=True)\n",
              "  (fc4): Linear(in_features=2, out_features=256, bias=True)\n",
              "  (fc5): Linear(in_features=256, out_features=512, bias=True)\n",
              "  (fc6): Linear(in_features=512, out_features=784, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRB6C6-tYQ46"
      },
      "source": [
        "optimizer = optim.Adam(vae.parameters())\n",
        "# return reconstruction error + KL divergence losses\n",
        "def loss_function(recon_x, x, mu, log_var):\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "    return BCE + KLD"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8T2kfriYXXC"
      },
      "source": [
        "losses_array = []\n",
        "def train(epoch):\n",
        "    vae.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        data = data.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        recon_batch, mu, log_var = vae(data)\n",
        "        loss = loss_function(recon_batch, data, mu, log_var)\n",
        "        \n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
        "    losses_array.append(train_loss / len(train_loader.dataset))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj6goi_OYa0Z"
      },
      "source": [
        "def test():\n",
        "    vae.eval()\n",
        "    test_loss= 0\n",
        "    with torch.no_grad():\n",
        "        for data, _ in test_loader:\n",
        "            data = data.cuda()\n",
        "            recon, mu, log_var = vae(data)\n",
        "            \n",
        "            # sum up batch loss\n",
        "            test_loss += loss_function(recon, data, mu, log_var).item()\n",
        "        \n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss))"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9seGko8Yd_2",
        "outputId": "06e7ba87-04b0-41c6-b0f2-876a25d9eade"
      },
      "source": [
        "for epoch in range(1, 51):\n",
        "    train(epoch)\n",
        "    test()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 156.737324\n",
            "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 165.937207\n",
            "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 161.552627\n",
            "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 146.525146\n",
            "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 151.420273\n",
            "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 154.246846\n",
            "====> Epoch: 1 Average loss: 152.5621\n",
            "====> Test set loss: 150.8660\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 142.451445\n",
            "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 149.202041\n",
            "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 143.277314\n",
            "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 154.521201\n",
            "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 150.791992\n",
            "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 143.826211\n",
            "====> Epoch: 2 Average loss: 149.2882\n",
            "====> Test set loss: 148.6284\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 147.347148\n",
            "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 143.189424\n",
            "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 145.919951\n",
            "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 145.492549\n",
            "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 154.294180\n",
            "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 153.708301\n",
            "====> Epoch: 3 Average loss: 147.2814\n",
            "====> Test set loss: 147.3826\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 143.120762\n",
            "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 144.395576\n",
            "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 141.153857\n",
            "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 152.260322\n",
            "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 141.666240\n",
            "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 146.074736\n",
            "====> Epoch: 4 Average loss: 145.8927\n",
            "====> Test set loss: 145.8074\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 138.680371\n",
            "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 140.971250\n",
            "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 139.464229\n",
            "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 146.200303\n",
            "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 149.713652\n",
            "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 145.179629\n",
            "====> Epoch: 5 Average loss: 144.8127\n",
            "====> Test set loss: 144.8922\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 141.867939\n",
            "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 138.680615\n",
            "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 147.356338\n",
            "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 140.839609\n",
            "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 147.089395\n",
            "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 146.480771\n",
            "====> Epoch: 6 Average loss: 144.0036\n",
            "====> Test set loss: 145.1232\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 145.923867\n",
            "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 149.493535\n",
            "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 144.981875\n",
            "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 144.152178\n",
            "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 140.823369\n",
            "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 141.012041\n",
            "====> Epoch: 7 Average loss: 143.1649\n",
            "====> Test set loss: 144.4047\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 144.253779\n",
            "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 141.138105\n",
            "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 140.387500\n",
            "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 153.484668\n",
            "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 145.613154\n",
            "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 149.661523\n",
            "====> Epoch: 8 Average loss: 142.6215\n",
            "====> Test set loss: 143.2633\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 153.520820\n",
            "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 140.875420\n",
            "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 131.851914\n",
            "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 140.217715\n",
            "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 144.181865\n",
            "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 142.868301\n",
            "====> Epoch: 9 Average loss: 142.0402\n",
            "====> Test set loss: 143.1491\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 133.338359\n",
            "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 131.140908\n",
            "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 138.269629\n",
            "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 135.225947\n",
            "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 138.785811\n",
            "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 134.394062\n",
            "====> Epoch: 10 Average loss: 141.5620\n",
            "====> Test set loss: 142.9024\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 146.902656\n",
            "Train Epoch: 11 [10000/60000 (17%)]\tLoss: 142.117100\n",
            "Train Epoch: 11 [20000/60000 (33%)]\tLoss: 132.657344\n",
            "Train Epoch: 11 [30000/60000 (50%)]\tLoss: 136.033047\n",
            "Train Epoch: 11 [40000/60000 (67%)]\tLoss: 143.393896\n",
            "Train Epoch: 11 [50000/60000 (83%)]\tLoss: 138.680693\n",
            "====> Epoch: 11 Average loss: 141.0649\n",
            "====> Test set loss: 142.2093\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 143.101699\n",
            "Train Epoch: 12 [10000/60000 (17%)]\tLoss: 145.210537\n",
            "Train Epoch: 12 [20000/60000 (33%)]\tLoss: 146.415430\n",
            "Train Epoch: 12 [30000/60000 (50%)]\tLoss: 147.128330\n",
            "Train Epoch: 12 [40000/60000 (67%)]\tLoss: 143.420342\n",
            "Train Epoch: 12 [50000/60000 (83%)]\tLoss: 134.459834\n",
            "====> Epoch: 12 Average loss: 140.7334\n",
            "====> Test set loss: 141.7020\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 138.374375\n",
            "Train Epoch: 13 [10000/60000 (17%)]\tLoss: 140.506084\n",
            "Train Epoch: 13 [20000/60000 (33%)]\tLoss: 138.861318\n",
            "Train Epoch: 13 [30000/60000 (50%)]\tLoss: 143.125205\n",
            "Train Epoch: 13 [40000/60000 (67%)]\tLoss: 140.367646\n",
            "Train Epoch: 13 [50000/60000 (83%)]\tLoss: 137.620420\n",
            "====> Epoch: 13 Average loss: 140.2653\n",
            "====> Test set loss: 141.2863\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 141.699814\n",
            "Train Epoch: 14 [10000/60000 (17%)]\tLoss: 134.132100\n",
            "Train Epoch: 14 [20000/60000 (33%)]\tLoss: 141.180820\n",
            "Train Epoch: 14 [30000/60000 (50%)]\tLoss: 136.904131\n",
            "Train Epoch: 14 [40000/60000 (67%)]\tLoss: 139.844658\n",
            "Train Epoch: 14 [50000/60000 (83%)]\tLoss: 136.072012\n",
            "====> Epoch: 14 Average loss: 140.0773\n",
            "====> Test set loss: 141.8039\n",
            "Train Epoch: 15 [0/60000 (0%)]\tLoss: 140.540918\n",
            "Train Epoch: 15 [10000/60000 (17%)]\tLoss: 134.459082\n",
            "Train Epoch: 15 [20000/60000 (33%)]\tLoss: 142.503018\n",
            "Train Epoch: 15 [30000/60000 (50%)]\tLoss: 138.458828\n",
            "Train Epoch: 15 [40000/60000 (67%)]\tLoss: 145.499482\n",
            "Train Epoch: 15 [50000/60000 (83%)]\tLoss: 144.225977\n",
            "====> Epoch: 15 Average loss: 139.5017\n",
            "====> Test set loss: 140.7298\n",
            "Train Epoch: 16 [0/60000 (0%)]\tLoss: 134.437920\n",
            "Train Epoch: 16 [10000/60000 (17%)]\tLoss: 147.020977\n",
            "Train Epoch: 16 [20000/60000 (33%)]\tLoss: 140.610615\n",
            "Train Epoch: 16 [30000/60000 (50%)]\tLoss: 146.475205\n",
            "Train Epoch: 16 [40000/60000 (67%)]\tLoss: 135.225273\n",
            "Train Epoch: 16 [50000/60000 (83%)]\tLoss: 143.347979\n",
            "====> Epoch: 16 Average loss: 139.3451\n",
            "====> Test set loss: 140.7074\n",
            "Train Epoch: 17 [0/60000 (0%)]\tLoss: 134.637061\n",
            "Train Epoch: 17 [10000/60000 (17%)]\tLoss: 137.639375\n",
            "Train Epoch: 17 [20000/60000 (33%)]\tLoss: 141.093389\n",
            "Train Epoch: 17 [30000/60000 (50%)]\tLoss: 137.730957\n",
            "Train Epoch: 17 [40000/60000 (67%)]\tLoss: 149.363242\n",
            "Train Epoch: 17 [50000/60000 (83%)]\tLoss: 142.848291\n",
            "====> Epoch: 17 Average loss: 139.2311\n",
            "====> Test set loss: 141.2531\n",
            "Train Epoch: 18 [0/60000 (0%)]\tLoss: 137.980723\n",
            "Train Epoch: 18 [10000/60000 (17%)]\tLoss: 135.368477\n",
            "Train Epoch: 18 [20000/60000 (33%)]\tLoss: 145.117002\n",
            "Train Epoch: 18 [30000/60000 (50%)]\tLoss: 130.132217\n",
            "Train Epoch: 18 [40000/60000 (67%)]\tLoss: 141.900635\n",
            "Train Epoch: 18 [50000/60000 (83%)]\tLoss: 133.722285\n",
            "====> Epoch: 18 Average loss: 138.9618\n",
            "====> Test set loss: 140.5630\n",
            "Train Epoch: 19 [0/60000 (0%)]\tLoss: 134.019238\n",
            "Train Epoch: 19 [10000/60000 (17%)]\tLoss: 135.846348\n",
            "Train Epoch: 19 [20000/60000 (33%)]\tLoss: 140.391172\n",
            "Train Epoch: 19 [30000/60000 (50%)]\tLoss: 137.497900\n",
            "Train Epoch: 19 [40000/60000 (67%)]\tLoss: 137.271260\n",
            "Train Epoch: 19 [50000/60000 (83%)]\tLoss: 137.995527\n",
            "====> Epoch: 19 Average loss: 138.7293\n",
            "====> Test set loss: 140.6138\n",
            "Train Epoch: 20 [0/60000 (0%)]\tLoss: 141.464004\n",
            "Train Epoch: 20 [10000/60000 (17%)]\tLoss: 138.195293\n",
            "Train Epoch: 20 [20000/60000 (33%)]\tLoss: 140.029805\n",
            "Train Epoch: 20 [30000/60000 (50%)]\tLoss: 141.072617\n",
            "Train Epoch: 20 [40000/60000 (67%)]\tLoss: 140.144795\n",
            "Train Epoch: 20 [50000/60000 (83%)]\tLoss: 143.285830\n",
            "====> Epoch: 20 Average loss: 138.4721\n",
            "====> Test set loss: 140.1211\n",
            "Train Epoch: 21 [0/60000 (0%)]\tLoss: 139.255713\n",
            "Train Epoch: 21 [10000/60000 (17%)]\tLoss: 144.511377\n",
            "Train Epoch: 21 [20000/60000 (33%)]\tLoss: 143.421182\n",
            "Train Epoch: 21 [30000/60000 (50%)]\tLoss: 144.844707\n",
            "Train Epoch: 21 [40000/60000 (67%)]\tLoss: 140.971729\n",
            "Train Epoch: 21 [50000/60000 (83%)]\tLoss: 144.109814\n",
            "====> Epoch: 21 Average loss: 138.2205\n",
            "====> Test set loss: 140.0351\n",
            "Train Epoch: 22 [0/60000 (0%)]\tLoss: 136.521475\n",
            "Train Epoch: 22 [10000/60000 (17%)]\tLoss: 130.761113\n",
            "Train Epoch: 22 [20000/60000 (33%)]\tLoss: 130.726055\n",
            "Train Epoch: 22 [30000/60000 (50%)]\tLoss: 138.144541\n",
            "Train Epoch: 22 [40000/60000 (67%)]\tLoss: 138.598105\n",
            "Train Epoch: 22 [50000/60000 (83%)]\tLoss: 140.358174\n",
            "====> Epoch: 22 Average loss: 138.2114\n",
            "====> Test set loss: 139.7997\n",
            "Train Epoch: 23 [0/60000 (0%)]\tLoss: 135.330957\n",
            "Train Epoch: 23 [10000/60000 (17%)]\tLoss: 138.210000\n",
            "Train Epoch: 23 [20000/60000 (33%)]\tLoss: 131.472686\n",
            "Train Epoch: 23 [30000/60000 (50%)]\tLoss: 130.192139\n",
            "Train Epoch: 23 [40000/60000 (67%)]\tLoss: 139.741660\n",
            "Train Epoch: 23 [50000/60000 (83%)]\tLoss: 141.333730\n",
            "====> Epoch: 23 Average loss: 137.8233\n",
            "====> Test set loss: 139.9873\n",
            "Train Epoch: 24 [0/60000 (0%)]\tLoss: 139.452227\n",
            "Train Epoch: 24 [10000/60000 (17%)]\tLoss: 138.047090\n",
            "Train Epoch: 24 [20000/60000 (33%)]\tLoss: 134.477383\n",
            "Train Epoch: 24 [30000/60000 (50%)]\tLoss: 137.165928\n",
            "Train Epoch: 24 [40000/60000 (67%)]\tLoss: 136.924463\n",
            "Train Epoch: 24 [50000/60000 (83%)]\tLoss: 136.759424\n",
            "====> Epoch: 24 Average loss: 137.5596\n",
            "====> Test set loss: 139.9283\n",
            "Train Epoch: 25 [0/60000 (0%)]\tLoss: 132.835010\n",
            "Train Epoch: 25 [10000/60000 (17%)]\tLoss: 142.789502\n",
            "Train Epoch: 25 [20000/60000 (33%)]\tLoss: 129.499570\n",
            "Train Epoch: 25 [30000/60000 (50%)]\tLoss: 142.074131\n",
            "Train Epoch: 25 [40000/60000 (67%)]\tLoss: 130.665547\n",
            "Train Epoch: 25 [50000/60000 (83%)]\tLoss: 126.898125\n",
            "====> Epoch: 25 Average loss: 137.4595\n",
            "====> Test set loss: 139.5633\n",
            "Train Epoch: 26 [0/60000 (0%)]\tLoss: 138.897373\n",
            "Train Epoch: 26 [10000/60000 (17%)]\tLoss: 137.353311\n",
            "Train Epoch: 26 [20000/60000 (33%)]\tLoss: 134.582031\n",
            "Train Epoch: 26 [30000/60000 (50%)]\tLoss: 136.318271\n",
            "Train Epoch: 26 [40000/60000 (67%)]\tLoss: 134.772734\n",
            "Train Epoch: 26 [50000/60000 (83%)]\tLoss: 144.236562\n",
            "====> Epoch: 26 Average loss: 137.1931\n",
            "====> Test set loss: 139.2825\n",
            "Train Epoch: 27 [0/60000 (0%)]\tLoss: 146.773740\n",
            "Train Epoch: 27 [10000/60000 (17%)]\tLoss: 142.079932\n",
            "Train Epoch: 27 [20000/60000 (33%)]\tLoss: 149.336445\n",
            "Train Epoch: 27 [30000/60000 (50%)]\tLoss: 137.516592\n",
            "Train Epoch: 27 [40000/60000 (67%)]\tLoss: 141.009824\n",
            "Train Epoch: 27 [50000/60000 (83%)]\tLoss: 141.991797\n",
            "====> Epoch: 27 Average loss: 137.2588\n",
            "====> Test set loss: 139.5511\n",
            "Train Epoch: 28 [0/60000 (0%)]\tLoss: 147.038496\n",
            "Train Epoch: 28 [10000/60000 (17%)]\tLoss: 143.390938\n",
            "Train Epoch: 28 [20000/60000 (33%)]\tLoss: 138.435947\n",
            "Train Epoch: 28 [30000/60000 (50%)]\tLoss: 133.827510\n",
            "Train Epoch: 28 [40000/60000 (67%)]\tLoss: 148.472021\n",
            "Train Epoch: 28 [50000/60000 (83%)]\tLoss: 131.244443\n",
            "====> Epoch: 28 Average loss: 137.1149\n",
            "====> Test set loss: 139.0182\n",
            "Train Epoch: 29 [0/60000 (0%)]\tLoss: 132.063066\n",
            "Train Epoch: 29 [10000/60000 (17%)]\tLoss: 134.930146\n",
            "Train Epoch: 29 [20000/60000 (33%)]\tLoss: 145.541875\n",
            "Train Epoch: 29 [30000/60000 (50%)]\tLoss: 143.590986\n",
            "Train Epoch: 29 [40000/60000 (67%)]\tLoss: 136.222734\n",
            "Train Epoch: 29 [50000/60000 (83%)]\tLoss: 130.109229\n",
            "====> Epoch: 29 Average loss: 137.0733\n",
            "====> Test set loss: 139.1524\n",
            "Train Epoch: 30 [0/60000 (0%)]\tLoss: 140.072197\n",
            "Train Epoch: 30 [10000/60000 (17%)]\tLoss: 130.366846\n",
            "Train Epoch: 30 [20000/60000 (33%)]\tLoss: 133.012451\n",
            "Train Epoch: 30 [30000/60000 (50%)]\tLoss: 134.306992\n",
            "Train Epoch: 30 [40000/60000 (67%)]\tLoss: 135.347363\n",
            "Train Epoch: 30 [50000/60000 (83%)]\tLoss: 146.948975\n",
            "====> Epoch: 30 Average loss: 136.6399\n",
            "====> Test set loss: 139.3898\n",
            "Train Epoch: 31 [0/60000 (0%)]\tLoss: 133.606963\n",
            "Train Epoch: 31 [10000/60000 (17%)]\tLoss: 135.086758\n",
            "Train Epoch: 31 [20000/60000 (33%)]\tLoss: 135.132666\n",
            "Train Epoch: 31 [30000/60000 (50%)]\tLoss: 134.827549\n",
            "Train Epoch: 31 [40000/60000 (67%)]\tLoss: 146.339180\n",
            "Train Epoch: 31 [50000/60000 (83%)]\tLoss: 131.267617\n",
            "====> Epoch: 31 Average loss: 136.7448\n",
            "====> Test set loss: 138.6752\n",
            "Train Epoch: 32 [0/60000 (0%)]\tLoss: 134.803760\n",
            "Train Epoch: 32 [10000/60000 (17%)]\tLoss: 136.713320\n",
            "Train Epoch: 32 [20000/60000 (33%)]\tLoss: 136.195547\n",
            "Train Epoch: 32 [30000/60000 (50%)]\tLoss: 133.838867\n",
            "Train Epoch: 32 [40000/60000 (67%)]\tLoss: 136.673652\n",
            "Train Epoch: 32 [50000/60000 (83%)]\tLoss: 132.268770\n",
            "====> Epoch: 32 Average loss: 136.5985\n",
            "====> Test set loss: 139.3827\n",
            "Train Epoch: 33 [0/60000 (0%)]\tLoss: 137.874814\n",
            "Train Epoch: 33 [10000/60000 (17%)]\tLoss: 144.650449\n",
            "Train Epoch: 33 [20000/60000 (33%)]\tLoss: 128.308818\n",
            "Train Epoch: 33 [30000/60000 (50%)]\tLoss: 143.261621\n",
            "Train Epoch: 33 [40000/60000 (67%)]\tLoss: 142.640205\n",
            "Train Epoch: 33 [50000/60000 (83%)]\tLoss: 134.989531\n",
            "====> Epoch: 33 Average loss: 136.4835\n",
            "====> Test set loss: 138.8994\n",
            "Train Epoch: 34 [0/60000 (0%)]\tLoss: 136.797129\n",
            "Train Epoch: 34 [10000/60000 (17%)]\tLoss: 136.442568\n",
            "Train Epoch: 34 [20000/60000 (33%)]\tLoss: 134.387783\n",
            "Train Epoch: 34 [30000/60000 (50%)]\tLoss: 134.338486\n",
            "Train Epoch: 34 [40000/60000 (67%)]\tLoss: 139.884043\n",
            "Train Epoch: 34 [50000/60000 (83%)]\tLoss: 134.390527\n",
            "====> Epoch: 34 Average loss: 136.1968\n",
            "====> Test set loss: 138.7148\n",
            "Train Epoch: 35 [0/60000 (0%)]\tLoss: 139.248955\n",
            "Train Epoch: 35 [10000/60000 (17%)]\tLoss: 141.210752\n",
            "Train Epoch: 35 [20000/60000 (33%)]\tLoss: 136.140586\n",
            "Train Epoch: 35 [30000/60000 (50%)]\tLoss: 133.254668\n",
            "Train Epoch: 35 [40000/60000 (67%)]\tLoss: 136.582607\n",
            "Train Epoch: 35 [50000/60000 (83%)]\tLoss: 142.198418\n",
            "====> Epoch: 35 Average loss: 136.0091\n",
            "====> Test set loss: 138.1651\n",
            "Train Epoch: 36 [0/60000 (0%)]\tLoss: 140.446035\n",
            "Train Epoch: 36 [10000/60000 (17%)]\tLoss: 137.075068\n",
            "Train Epoch: 36 [20000/60000 (33%)]\tLoss: 134.987686\n",
            "Train Epoch: 36 [30000/60000 (50%)]\tLoss: 133.957373\n",
            "Train Epoch: 36 [40000/60000 (67%)]\tLoss: 139.853652\n",
            "Train Epoch: 36 [50000/60000 (83%)]\tLoss: 120.592695\n",
            "====> Epoch: 36 Average loss: 136.1027\n",
            "====> Test set loss: 138.6482\n",
            "Train Epoch: 37 [0/60000 (0%)]\tLoss: 130.982891\n",
            "Train Epoch: 37 [10000/60000 (17%)]\tLoss: 133.567207\n",
            "Train Epoch: 37 [20000/60000 (33%)]\tLoss: 139.117139\n",
            "Train Epoch: 37 [30000/60000 (50%)]\tLoss: 133.898193\n",
            "Train Epoch: 37 [40000/60000 (67%)]\tLoss: 137.477529\n",
            "Train Epoch: 37 [50000/60000 (83%)]\tLoss: 138.411279\n",
            "====> Epoch: 37 Average loss: 136.0267\n",
            "====> Test set loss: 138.6836\n",
            "Train Epoch: 38 [0/60000 (0%)]\tLoss: 134.107949\n",
            "Train Epoch: 38 [10000/60000 (17%)]\tLoss: 133.809268\n",
            "Train Epoch: 38 [20000/60000 (33%)]\tLoss: 136.085459\n",
            "Train Epoch: 38 [30000/60000 (50%)]\tLoss: 141.096973\n",
            "Train Epoch: 38 [40000/60000 (67%)]\tLoss: 143.622617\n",
            "Train Epoch: 38 [50000/60000 (83%)]\tLoss: 132.305928\n",
            "====> Epoch: 38 Average loss: 135.7632\n",
            "====> Test set loss: 138.5363\n",
            "Train Epoch: 39 [0/60000 (0%)]\tLoss: 138.121113\n",
            "Train Epoch: 39 [10000/60000 (17%)]\tLoss: 136.188662\n",
            "Train Epoch: 39 [20000/60000 (33%)]\tLoss: 134.721416\n",
            "Train Epoch: 39 [30000/60000 (50%)]\tLoss: 135.238184\n",
            "Train Epoch: 39 [40000/60000 (67%)]\tLoss: 134.410332\n",
            "Train Epoch: 39 [50000/60000 (83%)]\tLoss: 132.118721\n",
            "====> Epoch: 39 Average loss: 135.5820\n",
            "====> Test set loss: 138.7573\n",
            "Train Epoch: 40 [0/60000 (0%)]\tLoss: 133.205400\n",
            "Train Epoch: 40 [10000/60000 (17%)]\tLoss: 133.164346\n",
            "Train Epoch: 40 [20000/60000 (33%)]\tLoss: 132.675273\n",
            "Train Epoch: 40 [30000/60000 (50%)]\tLoss: 136.154629\n",
            "Train Epoch: 40 [40000/60000 (67%)]\tLoss: 136.762031\n",
            "Train Epoch: 40 [50000/60000 (83%)]\tLoss: 143.572578\n",
            "====> Epoch: 40 Average loss: 135.4871\n",
            "====> Test set loss: 138.9597\n",
            "Train Epoch: 41 [0/60000 (0%)]\tLoss: 136.917969\n",
            "Train Epoch: 41 [10000/60000 (17%)]\tLoss: 129.243838\n",
            "Train Epoch: 41 [20000/60000 (33%)]\tLoss: 135.426875\n",
            "Train Epoch: 41 [30000/60000 (50%)]\tLoss: 132.119863\n",
            "Train Epoch: 41 [40000/60000 (67%)]\tLoss: 142.220605\n",
            "Train Epoch: 41 [50000/60000 (83%)]\tLoss: 132.695127\n",
            "====> Epoch: 41 Average loss: 135.5509\n",
            "====> Test set loss: 138.4884\n",
            "Train Epoch: 42 [0/60000 (0%)]\tLoss: 142.579980\n",
            "Train Epoch: 42 [10000/60000 (17%)]\tLoss: 140.795254\n",
            "Train Epoch: 42 [20000/60000 (33%)]\tLoss: 131.720107\n",
            "Train Epoch: 42 [30000/60000 (50%)]\tLoss: 136.700020\n",
            "Train Epoch: 42 [40000/60000 (67%)]\tLoss: 138.020293\n",
            "Train Epoch: 42 [50000/60000 (83%)]\tLoss: 142.275947\n",
            "====> Epoch: 42 Average loss: 135.3209\n",
            "====> Test set loss: 138.4706\n",
            "Train Epoch: 43 [0/60000 (0%)]\tLoss: 137.885762\n",
            "Train Epoch: 43 [10000/60000 (17%)]\tLoss: 140.584873\n",
            "Train Epoch: 43 [20000/60000 (33%)]\tLoss: 137.288945\n",
            "Train Epoch: 43 [30000/60000 (50%)]\tLoss: 128.474648\n",
            "Train Epoch: 43 [40000/60000 (67%)]\tLoss: 134.929639\n",
            "Train Epoch: 43 [50000/60000 (83%)]\tLoss: 123.612129\n",
            "====> Epoch: 43 Average loss: 135.2093\n",
            "====> Test set loss: 138.1791\n",
            "Train Epoch: 44 [0/60000 (0%)]\tLoss: 128.750645\n",
            "Train Epoch: 44 [10000/60000 (17%)]\tLoss: 130.466299\n",
            "Train Epoch: 44 [20000/60000 (33%)]\tLoss: 128.845488\n",
            "Train Epoch: 44 [30000/60000 (50%)]\tLoss: 134.409531\n",
            "Train Epoch: 44 [40000/60000 (67%)]\tLoss: 131.533535\n",
            "Train Epoch: 44 [50000/60000 (83%)]\tLoss: 139.270615\n",
            "====> Epoch: 44 Average loss: 135.0960\n",
            "====> Test set loss: 138.3342\n",
            "Train Epoch: 45 [0/60000 (0%)]\tLoss: 132.668936\n",
            "Train Epoch: 45 [10000/60000 (17%)]\tLoss: 136.273926\n",
            "Train Epoch: 45 [20000/60000 (33%)]\tLoss: 134.649082\n",
            "Train Epoch: 45 [30000/60000 (50%)]\tLoss: 132.097510\n",
            "Train Epoch: 45 [40000/60000 (67%)]\tLoss: 133.777998\n",
            "Train Epoch: 45 [50000/60000 (83%)]\tLoss: 138.316191\n",
            "====> Epoch: 45 Average loss: 135.1773\n",
            "====> Test set loss: 138.4690\n",
            "Train Epoch: 46 [0/60000 (0%)]\tLoss: 135.806826\n",
            "Train Epoch: 46 [10000/60000 (17%)]\tLoss: 134.378203\n",
            "Train Epoch: 46 [20000/60000 (33%)]\tLoss: 135.941855\n",
            "Train Epoch: 46 [30000/60000 (50%)]\tLoss: 139.624980\n",
            "Train Epoch: 46 [40000/60000 (67%)]\tLoss: 135.577314\n",
            "Train Epoch: 46 [50000/60000 (83%)]\tLoss: 131.512842\n",
            "====> Epoch: 46 Average loss: 135.2096\n",
            "====> Test set loss: 138.1846\n",
            "Train Epoch: 47 [0/60000 (0%)]\tLoss: 135.187920\n",
            "Train Epoch: 47 [10000/60000 (17%)]\tLoss: 137.239492\n",
            "Train Epoch: 47 [20000/60000 (33%)]\tLoss: 141.232607\n",
            "Train Epoch: 47 [30000/60000 (50%)]\tLoss: 134.713613\n",
            "Train Epoch: 47 [40000/60000 (67%)]\tLoss: 135.762490\n",
            "Train Epoch: 47 [50000/60000 (83%)]\tLoss: 134.160283\n",
            "====> Epoch: 47 Average loss: 134.9002\n",
            "====> Test set loss: 138.1524\n",
            "Train Epoch: 48 [0/60000 (0%)]\tLoss: 135.726475\n",
            "Train Epoch: 48 [10000/60000 (17%)]\tLoss: 134.401270\n",
            "Train Epoch: 48 [20000/60000 (33%)]\tLoss: 141.558584\n",
            "Train Epoch: 48 [30000/60000 (50%)]\tLoss: 130.305410\n",
            "Train Epoch: 48 [40000/60000 (67%)]\tLoss: 136.010156\n",
            "Train Epoch: 48 [50000/60000 (83%)]\tLoss: 133.879395\n",
            "====> Epoch: 48 Average loss: 134.7462\n",
            "====> Test set loss: 138.1601\n",
            "Train Epoch: 49 [0/60000 (0%)]\tLoss: 129.813047\n",
            "Train Epoch: 49 [10000/60000 (17%)]\tLoss: 131.671191\n",
            "Train Epoch: 49 [20000/60000 (33%)]\tLoss: 141.402178\n",
            "Train Epoch: 49 [30000/60000 (50%)]\tLoss: 132.782070\n",
            "Train Epoch: 49 [40000/60000 (67%)]\tLoss: 132.602100\n",
            "Train Epoch: 49 [50000/60000 (83%)]\tLoss: 131.048750\n",
            "====> Epoch: 49 Average loss: 134.7594\n",
            "====> Test set loss: 138.0627\n",
            "Train Epoch: 50 [0/60000 (0%)]\tLoss: 131.055371\n",
            "Train Epoch: 50 [10000/60000 (17%)]\tLoss: 132.069541\n",
            "Train Epoch: 50 [20000/60000 (33%)]\tLoss: 131.127334\n",
            "Train Epoch: 50 [30000/60000 (50%)]\tLoss: 137.768008\n",
            "Train Epoch: 50 [40000/60000 (67%)]\tLoss: 135.660840\n",
            "Train Epoch: 50 [50000/60000 (83%)]\tLoss: 136.298506\n",
            "====> Epoch: 50 Average loss: 134.9340\n",
            "====> Test set loss: 138.0356\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "00qToFS6rBN9",
        "outputId": "2e2e0af1-9a4b-48a2-e5a6-e783089fab06"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots()\n",
        "losses = np.array(losses_array)\n",
        "plt.plot(losses, label='VAE')\n",
        "plt.title(\"Training Losses\")\n",
        "plt.legend()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f99ade3a4a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEICAYAAABWJCMKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXiV9Z338fc3GyEkEEjCmoSwIwKmJaK4glq36qDVUqhbH3WoVseO1drleUacVudqbavVqcvQarVTC1itVau4jCK4oUYEAdlXEwiBsCQBErJ8nz/ODXPEBJKQ5CTnfF7XdS7O/buX873beD7n/v3uxdwdERGJPXGRLkBERCJDASAiEqMUACIiMUoBICISoxQAIiIxSgEgIhKjFAAStcxsrpld09rLikQL03UA0pGYWWXYZApQDdQF099196fav6qWM7OJwJ/dPTvStYgcLiHSBYiEc/fUg+/NbCNwvbv/z+HLmVmCu9e2Z20i0UZdQNIpmNlEMysysx+ZWQnwRzPraWb/MLPtZrYreJ8dts5bZnZ98P47ZvaOmf06WHaDmV3QwmUHmdkCM6sws/8xs4fM7M8t2Kfjgs/dbWbLzeyfwuZdaGafBZ9RbGa3B+2ZwX7uNrOdZva2mcUF8/qb2bPB/x4bzOyWsO2NN7NCMys3s21mdl9z65XoowCQzqQv0AsYCEwn9Pf7x2A6F9gP/O4I658ErAIygXuBx8zMWrDsX4APgQzgLuCq5u6ImSUCLwKvAb2BfwGeMrMRwSKPEerySgNGA28G7bcBRUAW0Af4KeBBCLwILAEGAGcD/2pm5wXrPQA84O7dgSHA082tWaKPAkA6k3pghrtXu/t+dy9z92fdfZ+7VwD3AGceYf1N7v57d68DngT6EfoSbfKyZpYLnAjc6e4H3P0d4IUW7MvJQCrwi2A7bwL/AKYF82uAUWbW3d13ufuisPZ+wEB3r3H3tz00kHcikOXuPwu2tx74PTA1bL2hZpbp7pXuvrAFNUuUUQBIZ7Ld3asOTphZipn9l5ltMrNyYAGQbmbxjaxfcvCNu+8L3qY2c9n+wM6wNoDPm7kfBNv53N3rw9o2Efr1DnAZcCGwyczmm9mEoP1XwFrgNTNbb2Y/DtoHAv2DrqHdZrab0NHBwYC7DhgOrDSzj8zsohbULFFGg8DSmRx+ytptwAjgJHcvMbN84BOgsW6d1rAV6GVmKWEhkNOC7WwBcswsLiwEcoHVAO7+ETA56Cq6mVCXTU5wpHMbcJuZjQbeNLOPCIXQBncf1tCHufsaYFrQVfQN4Bkzy3D3vS2oXaKEjgCkM0sj1O+/28x6ATPa+gPdfRNQCNxlZknBL/OLj7aemSWHvwiNIewD7jCzxOB00YuB2cF2rzCzHu5eA5QT6v7CzC4ys6HBeMQeQqfI1gfbqwgGybuaWbyZjTazE4P1rjSzrCBsdgdlhR99SAxSAEhn9lugK7ADWAi80k6fewUwASgD7gbmELpeoTEDCAVV+CuH0Bf+BYTqfxi42t1XButcBWwMurZuCD4TYBjwP0Al8D7wsLvPC8YqLgLygQ3BNv8A9AjWOx9YHlxn8QAw1d33H8P/BhIFdCGYyDEysznASndv8yMQkdakIwCRZjKzE81siJnFmdn5wGTg75GuS6S5NAgs0nx9gb8Rug6gCLjR3T+JbEkizacuIBGRGKUuIBGRGNWpuoAyMzM9Ly8v0mWIiHQqH3/88Q53zzq8vVMFQF5eHoWFhZEuQ0SkUzGzTQ21qwtIRCRGKQBERGKUAkBEJEZ1qjEAEZFjVVNTQ1FREVVVVUdfuJNJTk4mOzubxMTEJi2vABCRmFJUVERaWhp5eXk0/jygzsfdKSsro6ioiEGDBjVpHXUBiUhMqaqqIiMjI6q+/AHMjIyMjGYd2Rw1AMzscTMrNbNlYW13Bc8pXRy8Lgzav2ZmH5vZ0uDfsxrZZoPri4i0h2j78j+oufvVlCOAJwjdSvZw97t7fvB6OWjbAVzs7mOAa4D/PsJ2G1q/Tby5chsPv7W2LT9CRKTTOWoAuPsCYGdTNubun7j7lmByOdDVzLocQ32t4r21ZTzwP2uoq9d9j0QksiZNmsSrr776hbbf/va33HjjjezYsYPExEQeffTRL8zPy8tjzJgx5Ofnk5+fzy233NIqtRzLGMDNZvZp0EXUs4H5lwGL3L2xB2UcbX0AzGy6mRWaWeH27dtbVOjwPmlU19bz+c59R19YRKQNTZs2jdmzZ3+hbfbs2UybNo2//vWvnHzyycyaNetL682bN4/FixezePFiHnzwwVappaUB8AgwhNDTh7YCvwmfaWbHA78EvtuS9cO5+0x3L3D3gqysL93KokmG900DYNW2ihatLyLSWi6//HJeeuklDhw4AMDGjRvZsmULp59+OrNmzeI3v/kNxcXFFBUVtXktLToN1N23HXxvZr8H/hE2nQ08R+jxduuau35bGNY7FYDVJRWcd3zftvwoEelE/v3F5Xy2pbxVtzmqf3dmXHx8o/N79erF+PHjmTt3LpMnT2b27NlMmTKFoqIitm7dyvjx45kyZQpz5szhtttuO7TepEmTiI+PB+Caa67h1ltvPeZaW3QEYGb9wiYvBZYF7enAS8CP3f3d5q7fVrp1SSCnV1cdAYhIhxDeDXSw+2fOnDlMmTIFgKlTp36pGyi8C6g1vvyhCUcAZjYLmAhkmlkRMAOYaGb5gAMb+d+unpuBocCdZnZn0Hauu5ea2R+AR929ELi3kfXbzPDeaaxWAIhImCP9Um9LkydP5tZbb2XRokXs27ePcePGMX36dEpKSnjqqacA2LJlC2vWrGHYsGFtVsdRA8DdpzXQ/Fgjy94N3N3IvOvD3l/V1AJby/C+acxfvZ0DtfUkJej6NxGJnNTUVCZNmsS1117LtGnTWL16NZWVlRQXFx9aZsaMGcyaNYs777zzCFs6NjHzTTiiTxq19c7Gsr2RLkVEhGnTprFkyRKmTZvGrFmzuPTSS78w/7LLLvtCN9CkSZMOnQZ69dVXt0oNMXMvoOF9gjOBSioOvRcRiZRLLrmEg89knzFjxpfmjx07lhUrVgChM4XaQswcAQzO6kZ8nGkcQEQkEDMBkJwYz8CMFFaVKABERCCGAgBC4wBrSisjXYaIRNjBrpdo09z9iqkAGN4njY1le6mqqYt0KSISIcnJyZSVlUVdCBx8HkBycnKT14mZQWCAEX3TcIe1pZWMHtAj0uWISARkZ2dTVFRES+8t1pEdfCJYU8VUAISfCaQAEIlNiYmJTX5iVrSLqS6gvIwUkuLjWF2qgWARkZgKgIT4OAZndWO1zgQSEYmtAIDQOMDqbToTSEQk5gJgeJ80infvp6KqJtKliIhEVEwGAKCjABGJeTEXACOCAFijW0KISIyLuQDI7tmVronxejiMiMS8mAuAuDhjeJ9U3RRORGJezAUAhMYBVpVoDEBEYlvMBsCOymp27j0Q6VJERCImNgOg78EzgdQNJCKxq0kBYGaPm1mpmS0La7vLzIrNbHHwujBs3k/MbK2ZrTKz8xrZ5iAz+yBYbo6ZJR377jTNiD4KABGRph4BPAGc30D7/e6eH7xeBjCzUcBU4PhgnYfNLL6BdX8ZrD8U2AVc19ziW6pP9y50T07Qw2FEJKY1KQDcfQGws4nbnAzMdvdqd98ArAXGhy9gZgacBTwTND0JXNLE7R8zMwtuCaEAEJHYdaxjADeb2adBF1HPoG0A8HnYMkVBW7gMYLe71x5hGQDMbLqZFZpZYWvev3tYn9A9gaLtoRAiIk11LAHwCDAEyAe2Ar9plYoO4+4z3b3A3QuysrJabbsj+qSxZ38NpRXVrbZNEZHOpMUB4O7b3L3O3euB3/O/3TzFQE7YotlBW7gyIN3MEo6wTJsKfziMiEgsanEAmFm/sMlLgYNnCL0ATDWzLmY2CBgGfBi+rof6XeYBlwdN1wDPt7SWlhjeJxXQmUAiErua9EhIM5sFTAQyzawImAFMNLN8wIGNwHcB3H25mT0NfAbUAje5e12wnZeB6919C/AjYLaZ3Q18AjzWivt1VBmpXchMTdIRgIjErCYFgLtPa6C50S9sd78HuKeB9gvD3q/nsLOD2tvwPmmsLtUtIUQkNsXklcAHDe+TxpptFdTX60wgEYk9MR0AI/umse9AHRvL9ka6FBGRdhfTAXDS4AwA3lm7I8KViIi0v5gOgLyMFHJ7pTB/VetdYCYi0lnEdACYGWcMz+T99WUcqK2PdDkiIu0qpgMA4Mzhvdl3oI7CTU291ZGISHSI+QCYMCSDhDhj/mp1A4lIbIn5AEjtksC4gT1ZsFoDwSISW2I+AADOHJHFiq3llJZXRboUEZF2owAAzhgWusvogjU6ChCR2KEAAEb1605mahcWaBxARGKIAgCIizPOGJbJ22u2U6fbQohIjFAABM4YnsWufTUsK94T6VJERNqFAiBw+rBMzFA3kIjEDAVAICO1C6P799D1ACISMxQAYc4cnsUnn++mvKom0qWIiLQ5BUCYM4ZnUVfvvKe7g4pIDFAAhPlKbjppXRLUDSQiMeGoAWBmj5tZqZkta2DebWbmZpYZTP/QzBYHr2VmVmdmvRpY7wkz2xC2bH7r7M6xSYyP45ShGSxYvYPQc+tFRKJXU44AngDOP7zRzHKAc4HNB9vc/Vfunu/u+cBPgPnu3thtNn94cFl3X9z80tvGGcOzKN69n3Xb9axgEYluRw0Ad18ANPQlfj9wB9DYT+VpwKyWlxYZB28LMV83hxORKNeiMQAzmwwUu/uSRuanEDpqePYIm7nHzD41s/vNrMsRPmu6mRWaWeH27W3fN5/TK4XBWd10PYCIRL1mB0Dw5f5T4M4jLHYx8O4Run9+AowETgR6AT9qbEPuPtPdC9y9ICsrq7nltsiZw7NYuL6Mqpq6dvk8EZFIaMkRwBBgELDEzDYC2cAiM+sbtsxUjtD94+5bPaQa+CMwvgV1tJkzhmdRXVvP++vLIl2KiEibaXYAuPtSd+/t7nnungcUAV919xIAM+sBnAk839g2zKxf8K8BlwBfOsMokiYMzqBnSiLPFBZFuhQRkTbTlNNAZwHvAyPMrMjMrjvKKpcCr7n73sO287KZ9Q8mnzKzpcBSIBO4u/mlt53kxHgu+2o2ry4vobRCD4kRkehknel894KCAi8sLGyXz1q3vZKzfzOfH543gpsmDW2XzxQRaQtm9rG7FxzeriuBGzEkK5UJgzOY/dFm6vWMABGJQgqAI/j2Sbl8vnM/b+veQCIShRQAR3De8X3J6JbEUws3RboUEZFWpwA4gqSEOC4vyOaNlaVsK9dgsIhEFwXAUUw7MZe6emfOR59HuhQRkValADiKvMxunDY0k9kfbtYD40UkqigAmuCKk3LZsqeK+atLI12KiEirUQA0wTmj+pCV1oWnFm4++sIiIp2EAqAJEuPjmFKQzbxVpWzZvT/S5YiItAoFQBNNPTEXB2ZrMFhEooQCoIlyeqVwxrAs5ny0mdq6+kiXIyJyzBQAzfDtk3LZVl7NGys1GCwinZ8CoBnOHtmbAeldeXT+Oj00XkQ6PQVAMyTEx3HzWUP5ZPNu3tRRgIh0cgqAZrp8XDYDM1L49WurdZdQEenUFADNlBgfx63nDGfF1nLmLiuJdDkiIi2mAGiBi0/oz7Deqdz3+irdHkJEOi0FQAvExxm3nTucddv38twnxZEuR0SkRRQALXTe8X0ZPaA7D7yxmgO1ui5ARDqfJgWAmT1uZqVmtqyBebeZmZtZZjA90cz2mNni4HVnI9scZGYfmNlaM5tjZknHtivty8y47dwRfL5zP08X6upgEel8mnoE8ARw/uGNZpYDnAscfpe0t909P3j9rJFt/hK4392HAruA65pYS4cxcXgWBQN78p9vrqGqpi7S5YiINEuTAsDdFwA7G5h1P3AH0KyRUDMz4CzgmaDpSeCS5myjIzAzbj9vBNvKq/mzHhspIp1Mi8cAzGwyUOzuSxqYPcHMlpjZXDM7voH5GcBud68NpouAAY18znQzKzSzwu3bt7e03DZz8uAMThuaySNvrWNvde3RVxAR6SBaFABmlgL8FGiof38RMNDdTwD+E/h7y8sDd5/p7gXuXpCVlXUsm2ozt583grK9B3j8nQ2RLkVEpMlaegQwBBgELDGzjUA2sMjM+rp7ubtXArj7y0DiwQHiMGVAupklBNPZQKc9nzI/J53zju/DQ2+tZd32ykiXIyLSJC0KAHdf6u693T3P3fMIdeF81d1LzKxv0MePmY0PPqPssPUdmAdcHjRdAzzfwn3oEH4+eTRdE+P5wZzF1Oh20SLSCTT1NNBZwPvACDMrMrMjnbFzObDMzJYADwJTgy98zOxlM+sfLPcj4AdmtpbQmMBjLd2JjqB392TuuXQMS4r28NC8tZEuR0TkqKwz3da4oKDACwsLI13GEd06ZzEvLNnCszeeQn5OeqTLERHBzD5294LD23UlcCu765+Op3daF34wZzH7D+jaABHpuBQAraxH10R+880TWL9jL7+YuyLS5YiINEoB0AZOGZrJtacO4sn3N7Fgdce7dkFEBBQAbeaO80cwtHcqP3xmCbv3HYh0OSIiX6IAaCPJifH89lv5lFUe4N+eXx7pckREvkQB0IZGD+jB988exotLtvDy0q2RLkdE5AsUAG3sxolDGDOgB//292WUVVZHuhwRkUMUAG0sIT6OX31zLOVVNdz5grqCRKTjUAC0g5F9u/P9s4fx0qdb1RUkIh2GAqCd3HCmuoJEpGNRALSThPg4fv3NE6ioquVOnRUkIh2AAqAdjeibxvfPGcZLS7fy0qfqChKRyFIAtLPvnjGYMQN6cOfz6goSkchSALQzdQWJSEehAIiA8K6g+15fTWe6JbeIRI+Eoy8ibeGGM4ewqWwvD76xhgO19fzo/BEED1ITEWkXCoAIiY8zfvGNsSQlxPHo/HVU19Zx50WjFAIi0m4UABEUF2f8fPJokuLjefzdDRyorefnk0cTF6cQEJG2pwCIMDPj3y46ji6JcTzy1joO1Nbzi8vGEq8QEJE2dtRBYDN73MxKzWxZA/NuMzM3s8xg+goz+9TMlprZe2Z2QiPbfMLMNpjZ4uCVf+y70nmZGXecN4Lvnz2Mv35cxA+eXkxtXX2kyxKRKNeUI4AngN8BfwpvNLMc4Fxgc1jzBuBMd99lZhcAM4GTGtnuD939mWZXHKXMjFu/NpykhDh+9eoqAO6bkq8jARFpM0cNAHdfYGZ5Dcy6H7gDeD5s2ffC5i8Eso+xvphz06ShxJnxy1dW0iUhjl98Y6zGBESkTbToOgAzmwwUu/uSIyx2HTD3CPPvCbqL7jezLkf4rOlmVmhmhdu3x8bzdW+cOIRbzh7G04VF3PXicl0nICJtotmDwGaWAvyUUPdPY8tMIhQApzWyyE+AEiCJUDfRj4CfNbSgu88MlqGgoCBmvglvPWcYVTV1zFywnuTEeH5ywUidIioiraolZwENAQYBS4IvpGxgkZmNd/cSMxsL/AG4wN3LGtqAux+8E1q1mf0RuL0FdUQ1M+MnF4z8Qgj84GvDI12WiESRZgeAuy8Feh+cNrONQIG77zCzXOBvwFXuvrqxbZhZP3ffaqEEuQT40hlGEgqBuy4+nuqaeh58Yw3JiXF8b+LQSJclIlHiqAFgZrOAiUCmmRUBM9z9sUYWvxPIAB4Ojg5q3b0g2M7LwPXuvgV4ysyyAAMWAzcc645Eq7g44z++MYaq2jrufWUVhnHDmYPVHSQix8w60wBjQUGBFxYWRrqMiKitq+fWp5fw4pItTBufy88mH09ivO7lJyJHZ2YfH/wxHk5XAncSCfFxPPCtfHJ7deWheeso2rWPh674Kt2TEyNdmoh0UvoJ2YnExRk/PG8k914+lvfXlXHZw+/x+c59kS5LRDopBUAnNKUghz9dN55t5VVc+vC7LNq8K9IliUgnpADopE4ZksnfvncqKUkJTJu5UM8YFpFmUwB0YkN7p/L3m05lzIAe3PSXRfzh7fWRLklEOhEFQCfXq1sSf77+JC4Y3Ze7X1rBz//xGfX1nefMLhGJHAVAFEhOjOd33/4q3zklj8fe2cC/zP6E6tq6SJclIh2cTgONEvFxxoyLR9E/PZn/eHklOyqqmXlVAT1SdJqoiDRMRwBRxMyYfsYQHpiaz6LNu/jmf73Hlt37I12WiHRQCoAoNDl/AE/+n/Fs3V3F5Ife5YUlW3RLaRH5EgVAlDplaCZ/vXECfbp34ZZZnzB15kJWlpRHuiwR6UAUAFFsZN/uPH/TafzHpWNYta2Crz/4Dne9sJw9+2siXZqIdAAKgCgXH2d8+6Rc5t02kWnjc/jT+xs569dv8fRHn6tbSCTGKQBiRM9uSdx9yRheuPk08jK7ccezn3LL7MVU1eh0UZFYpQCIMaMH9OCZGybww/NG8OKSLXxr5kJKy6siXZaIRIACIAaZGTdNGsqjV45jdUkFkx96l2XFeyJdloi0MwVADDt/dF/+esMEAL756Pu8urwkwhWJSHtSAMS40QN68PxNpzK8bxrf/e+PefittRocFokRTQoAM3vczErN7EsPbzez28zMzSwzmDYze9DM1prZp2b21Ua2Oc7MlgbLPWh6yG3E9O6ezJzpJ3PxCf2595VVnH3ffO57bRUrS8oVBiJRrKlHAE8A5x/eaGY5wLnA5rDmC4BhwWs68Egj23wE+OewZb+0fWk/yYnxPDg1n19/8wT69Ujmd/PWcv5v3+ac++Zz3+urWb2tItIlikgra1IAuPsCYGcDs+4H7gDCfyZOBv7kIQuBdDPrF75SMN3d3Rd66Cfmn4BLWrID0nrMjMvHZfPU9Sfz4f89h7svGU3vtGR+9+Yazr1/Ad976mNq6uojXaaItJIW3w3UzCYDxe6+5LDemwHA52HTRUHb1sOWKWpgGekgMlO7cOXJA7ny5IGUVlTx1MLNPPDGGuATHpz6FRLiNXwk0tm1KADMLAX4KaHunzZlZtMJdSWRm5vb1h8nDeidlsytXxtOWnICd7+0gsT4Jdw3JZ/4OA3biHRmLT0CGAIMAg7++s8GFpnZeKAYyAlbNjtoC1cctB9pGQDcfSYwE6CgoEAjkhF0/emDqa6t51evriIxPo57LxtLnEJApNNqUQC4+1Kg98FpM9sIFLj7DjN7AbjZzGYDJwF73H3rYetvNbNyMzsZ+AC4GvjPFu6DtKObJg3lQG09D7yxhqSEOO65ZDQ6gUukc2pSAJjZLGAikGlmRcAMd3+skcVfBi4E1gL7gP8Ttp3F7p4fTH6P0NlFXYG5wUs6gX89ZxgH6up55K11JMXHMePiUQoBkU6oSQHg7tOOMj8v7L0DNzWyXH7Y+0JgdJOqlA7FzLjjvBEcqK3nsXc2UF5Vw5SCHMYN7EmiBodFOg09E1haxMz4f18/jjiDP767kb8tKia1SwIThmRw5vAszhyeRU6vlEiXKSJHYJ3pSs+CggIvLCyMdBlymPKqGt5bW8aCNduZv2o7xcFziPMyUsjPSWdMdjpjs3twfP/upCTpN4dIezOzj9294EvtCgBpTe7Ouu17WbB6O++vL2Np0R5KgttNxxkM653G2OweTB2fy7iBPSNcrUhsUABIxJSWV7G0eA+fFu1hafEeCjfupLyqlgmDM7j5rKGcMiRDg8gibUgBIB3GvgO1/OWDzcxcsJ7Simryc9K5edJQzj6ut4JApA0oAKTDqaqp45mPi3h0/jqKdu1nZN80/vn0wXx9bD+SE+MjXZ5I1FAASIdVU1fPC4u38Mj8dawtraRnSiJTTszhypMG6kwikVagAJAOz915b10Z//3+Jl5fsY16dyaN6M1VJw/kjOFZuveQSAspAKRT2bpnP7M+/JxZH25me0U1eRkpXH/6YC4fl63uIZFmUgBIp3Sgtp5Xl5fwh7fXs6RoDxndkvjOKXlcNWEg6SlJkS5PpFNQAEin5u4sXL+TmQvWMW/VdlKS4plSkMN1pw3SOIHIUSgAJGqsKqlg5oL1PL+4GAfOH92Xfz59MPk56ZEuTaRDUgBI1Nm6Zz9PvLuRv3ywmYrqWk7M68n1pw/mnOP6aMBYJIwCQKJWZXUtcz76nMff2UDx7v3kZaTw7ZNyyejWhYR4IyEujvg4IyHOSE6MZ/ygXiQl6K6lEjsUABL1auvqeWV5Cb9/ewNLPt/d6HLH9+/O/d/KZ3iftHasTiRyFAASM9ydbeXVHKitp7a+ntp6p7bOqat31u+o5GcvfkZFdS13nDeCa08dpMdaStRrLAB0b16JOmZG3x7JDc4bk92DU4dm8uNnl3L3Syt4Y0Upv55yAgPSu7ZzlSKRp45QiTmZqV34/dXjuPeysXxatJvz71/Ac58U0ZmOhkVagwJAYpKZMeXEHOZ+/wxG9kvj1jlLuOIPH/DOmh0KAokZRw0AM3vczErNbFlY28/N7FMzW2xmr5lZ/6D9h0HbYjNbZmZ1ZtargW0+YWYbwpbNP3wZkfaQm5HC7OkTuPOiUawtreTKxz5g8kPvMnfpVurrFQQS3Y46CGxmZwCVwJ/cfXTQ1t3dy4P3twCj3P2Gw9a7GLjV3c9qYJtPAP9w92eaU6wGgaUtVdfW8bdFxfzX/HVsLNvH4Kxu3HDGEC75ygCdNiqdWosHgd19gZnlHdZWHjbZDWgoRaYBs5pXpkjkdEmIZ9r4XKYU5DB32VYenreOO579lP/3/DKG90llZN/ujOybxnH9ujOibxqZqV0iXbLIMWnSaaBBAPzj4BFA0HYPcDWwB5jk7tvD5qUARcBQd9/ZwPaeACYA1cAbwI/dvbqRz54OTAfIzc0dt2nTpibumsixcXfeXrODd9buYMXWclaWVLC94n//THN6deW6UwfxrRNz6ZqkO5RKx3VM1wE0FABh834CJLv7jLC2bwFXuvvFjWyvH1ACJAEzgXXu/rOj1aEuIIm0sspqVpVUsKKkgrlLt1K4aRcZ3ZK49rRBXDVhIN2TEyNdosiXtGUA5AIvH3Z08BzwV3f/SxO2PRG43d0vOtqyCgDpaD7csJOH5q1l/urtpHVJ4OpTBnLNhDxq650tu/dTvHs/W3ZXsWX3fraVV2jyP0YAAAyTSURBVDG0dyqnDMlk3MCeOmqQdtOqAWBmw9x9TfD+X4Az3f3yYLoHsAHIcfe9jWyvn7tvtdATwO8Hqtz9x0erQwEgHdWy4j08/NZa5i4roaH/pHp0TSQjNYnNZfuorXeS4uPIz03nlCEZnDIkk2G9U+neNVE3sZM20eIAMLNZwEQgE9gGzAAuBEYA9cAm4AZ3Lw6W/w5wvrtPPWw7LwPXu/sWM3sTyAIMWBysX3m0nVAASEe3trSSV5eX0DMlif7pyQxI70q/9K6kdgmdb1FZXctHG3eycF0Z760rY9mWPYcCwywUFD1TkkhPCf07bmBPvnvGYBLidRaStJzuBSTSAe3ZV8OHG3fy+c597N53gF37ati17wC799Wwo7KalSUVnDSoF//57a/QO63h21uIHI3uBSTSAfVISeRro/o0Ov/Zj4v4v39fykUPvsNDV3yVE/O+dF2lSIvpuFKkA7tsXDbPfe9UUpLimTpzIX94e71uVSGtRkcAIh3ccf2688K/nMbtTy/h7pdWsGjzLu69/ASqaur4bEs5n20tP/Tv5p37GNWvO6cOzeDUIZl8dWBPkhN1tpE0TGMAIp2Eu/NfC9Zz7ysrSYyPo7q2/tC8AeldOa5fd7J7duXTot0sKdpDXb3TJSGOgryeh049HT2gx6EBaYkdGgMQ6eTMjBvOHEJ+TjovLtnCoMxujOrfnVH9upOekvSFZSuqavhww07eXVvGe+t28KtXVwXbgCFZqYzN7sEJ2emMye7B6P49dK+jGKUjAJEYUFZZzadFe4JX6AhhR2Xothb9eiRz81lD+ea4HAVBlNJpoCJyiLuzdU8Vizbv4vF3NrBo826ye3bllrOH8Y2vDNB1B1FGASAiDXJ33lq9nfteW83S4j0MyuzG988exsUn9NeVyVFCASAiR+TuvP7ZNu57fTUrSyrI7ZXCxSf04+tj+nNcvzRCd26RzkgBICJNUl/vzF1WwuyPNvPeujLq6p3Bmd34+th+XDimHyP7plFZXcua0krWbKtg9bZK1pRWsrlsLxmpXRjYK4WcXikMzAi9cnt1IytNz06IJAWAiDRbWWU1ry7fxktLt/D+ujLqHdJTEtm9r+bQMl0S4hjaO5W8jG7sqKxm8859lJRXfeGmePk56dw4cQhfO64PcepWancKABE5Jjsqq3llWQmfFu1mYEY3hvVOZXifNHJ6pXxprKCqpo6iXfvZvHMvq7dV8pcPNrN55z6G9k7lhjOHMDm/P4kaaG43CgARiZjaunpeWrqVR95ax8qSCvr3SOafzxjMt07MISVJlyO1NQWAiEScu/PWqu088tY6Pty4k66J8UwckcX5o/ty1sjepOmJam1CVwKLSMSZGZNG9mbSyN58vGknz31SzKvLtzF3WQlJ8XGcPizzUBhkpGrguK3pCEBEIqqu3lm0eRevLCvhlWUlFO/eD0DvtC4M75PGsD6hsYbhfdLI6dmV7ZXVbN1dxdY9+9myJ/S4zbLKA5wyNIMpBTlkKji+RF1AItLhuTtLi/ewcH0Zq7dVsnpbBWu2VbK/pq7B5RPjjT7dk0ntksDKkgoS440Lx/TjypMHUjCwp65dCKgLSEQ6PDNjbHY6Y7PTD7XV1zvFu/ezqqSCLXv2k5nahf7pXenfI5nM1C6HTitdW1rBnxdu5tlFRTy/eAsj+qRx5cm5nDe6r56m1ggdAYhIVNl3oJYXFm/hzx9sYllxORDqThozoAejB/RgzIAejMnuQZ/usRMKx3QEYGaPAxcBpe4+Omj7OTCZ0IPhS4HvBA98nwg8D2wIVv+bu/+sgW0OAmYDGcDHwFXufqC5OyYiEi4lKYGp43P51ok5LCsu58ONO1lWvIelxXt4c1XpoQvUeqd1YWx2D8YMSGdMdnfGDEj/whXLldW1bNyxlw3Bq2jXPvbsr6GiqpaKqloqq2upqKph/4E6zj2+L7efN4IB6V0jtNct06QjADM7A6gE/hQWAN3dvTx4fwswyt1vCALgdne/6CjbfJpQOMw2s0eBJe7+yJHW0RGAiByLvdW1fLa1nKVFew6FwtrtlYdCoV+PZPr1SGbzzv2Hbpd9UJ/uXUjvmkRacgJpyQmkJieSlpxAXZ3z3OJiAK49dRDfmzSE7h3sdNZjOgJw9wVmlndYW3nYZDegyX1JFhqZOQv4dtD0JHAXcMQAEBE5Ft26JHBiXi9OzOt1qK2yupbPtpTzadFulhbvYVt5FWeNzCIvsxuDM7uRl9mNgb260TWp8Udr3nLOMH7z6ioenb+Opws/55azhnLFyQNb7Wrnmrp64s1a/TYaTR4DCALgHwePAIK2e4CrgT3AJHffHhwBPAsUAVsIHQ0sP2xbmcBCdx8aTOcAc8O3HbbsdGA6QG5u7rhNmzY1cxdFRNrHsuI9/MfLK3hvXRmDMrtx7al5nDm8N7kZKc3e1r4DtSxYvZ3Xlm/jjZWlPHntePJz0o++YgOO+TTQhgIgbN5PgGR3n2Fm3YF6d680swuBB9x92GHLNzkAwqkLSEQ6uoNXO/9i7kpWbasAIC8jhdOHZXH6sEwmDMlo9IrnHZXVvLFiG69/to231+ygurae9JREzh7Zh+lnDGZE37QW1dTWp4E+BbwMzAjvGnL3l83sYTPLdPcdYcuXAelmluDutUA2UNxKtYiIRMzBq50njshiw469LFi9nbfX7ODZRUX898JNJMQZOb1SqK2vp6bWqamr50BdPTV19VTV1AMwIL0r3z4pl3NH9eXEvJ5t9oS2FgeAmQ1z9zXB5GRgZdDeF9jm7m5m44E4Ql/4hwTz5gGXEzoT6BpCZw6JiEQFM2NwViqDs1L5zqmDqK6tY9Gm3SxYs53NO/fRJT6OxPg4EhOMxPg4kuLj6N41kYkjshjVr3u7XMTW1NNAZwETgUwzKwJmABea2QhCp4FuAm4IFr8cuNHMaoH9wFQP+pnM7GXgenffAvwImG1mdwOfAI+12l6JiHQwXRLimTAkgwlDMiJdyiG6EExEJMo1NgagJzKIiMQoBYCISIxSAIiIxCgFgIhIjFIAiIjEKAWAiEiMUgCIiMSoTnUdgJltJ3TRWUtkAjuOulT00X7Hnljdd+134wa6e9bhjZ0qAI6FmRU2dCFEtNN+x55Y3Xftd/OpC0hEJEYpAEREYlQsBcDMSBcQIdrv2BOr+679bqaYGQMQEZEviqUjABERCaMAEBGJUTERAGZ2vpmtMrO1ZvbjSNfTVszscTMrNbNlYW29zOx1M1sT/NszkjW2BTPLMbN5ZvaZmS03s+8H7VG972aWbGYfmtmSYL//PWgfZGYfBH/vc8wsKdK1tgUzizezT8zsH8F01O+3mW00s6VmttjMCoO2Fv+dR30AmFk88BBwATAKmGZmoyJbVZt5Ajj/sLYfA2+4+zDgjWA62tQCt7n7KOBk4Kbg/+No3/dq4Cx3PwHIB843s5OBXwL3u/tQYBdwXQRrbEvfB1aETcfKfk9y9/ywc/9b/Hce9QEAjAfWuvt6dz9A6BnEkyNcU5tw9wXAzsOaJwNPBu+fBC5p16LagbtvdfdFwfsKQl8KA4jyffeQymAyMXg5cBbwTNAedfsNYGbZwNeBPwTTRgzsdyNa/HceCwEwAPg8bLooaIsVfdx9a/C+BOgTyWLampnlAV8BPiAG9j3oBlkMlAKvA+uA3e5eGywSrX/vvwXuIPRMcoAMYmO/HXjNzD42s+lBW4v/zpv0UHiJDu7uZha15/2aWSrwLPCv7l4e+lEYEq377u51QL6ZpQPPASMjXFKbM7OLgFJ3/9jMJka6nnZ2mrsXm1lv4HUzWxk+s7l/57FwBFAM5IRNZwdtsWKbmfUDCP4tjXA9bcLMEgl9+T/l7n8LmmNi3wHcfTcwD5gApJvZwR930fj3firwT2a2kVCX7lnAA0T/fuPuxcG/pYQCfzzH8HceCwHwETAsOEMgCZgKvBDhmtrTC8A1wftrgOcjWEubCPp/HwNWuPt9YbOiet/NLCv45Y+ZdQW+Rmj8Yx5webBY1O23u//E3bPdPY/Qf89vuvsVRPl+m1k3M0s7+B44F1jGMfydx8SVwGZ2IaE+w3jgcXe/J8IltQkzmwVMJHR72G3ADODvwNNALqFbaU9x98MHijs1MzsNeBtYyv/2Cf+U0DhA1O67mY0lNOgXT+jH3NPu/jMzG0zol3Ev4BPgSnevjlylbSfoArrd3S+K9v0O9u+5YDIB+Iu732NmGbTw7zwmAkBERL4sFrqARESkAQoAEZEYpQAQEYlRCgARkRilABARiVEKABGRGKUAEBGJUf8fxygSXAnQPRQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nV6cW5WSbTeE",
        "outputId": "24ceeb35-44f2-4100-a0d4-26a7b9a91adb"
      },
      "source": [
        "with torch.no_grad():\n",
        "    z = torch.randn(64, 2).cuda()\n",
        "    sample = vae.decoder(z).cuda()\n",
        "    \n",
        "    save_image(sample.view(64, 1, 28, 28), '_1' + '.png')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}